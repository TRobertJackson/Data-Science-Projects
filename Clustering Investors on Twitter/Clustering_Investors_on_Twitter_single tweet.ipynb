{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get recent tweets from 143 famous investors on Twitter\n",
    "\n",
    "The Best Invsting Blogs of 2017\n",
    "https://thecollegeinvestor.com/15601/the-best-investing-blogs/\n",
    "\n",
    "17 Venture Capital and Angel Investors to Follow on Twitter\n",
    "https://www.inc.com/larry-kim/17-venture-capital-and-angel-investors-to-follow-on-twitter.html\n",
    "\n",
    "Nine Twitter Accounts For Aspiring Real Estate Investors To Follow\n",
    "https://www.forbes.com/sites/forbesrealestatecouncil/2017/05/03/9-twitter-accounts-for-aspiring-real-estate-investors-to-follow/#61c1f6a627a0\n",
    "\n",
    "10 VCs & Angel Investors to Follow on Twitter\n",
    "http://www.techinsurance.com/blog/business-tips/10-vcs-angel-investors-to-follow-on-twitter/\n",
    "\n",
    "These Are The Top 20 Tech Investors You Should Follow On Twitter\n",
    "http://www.businessinsider.com/top-20-tech-investors-on-twitter-2013-5?op=1\n",
    "\n",
    "Top 50 Investors on Twitter and How To Engage Them\n",
    "http://startupfundraising.com/top-50-investors-on-twitter-and-how-to-engage-them/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = 81917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "investor_tweets_db = client.investor_tweets_db\n",
    "tweets_collection = investor_tweets_db.tweets_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = os.environ[\"TWITTER_CONSUMER_KEY\"]\n",
    "consumer_secret = os.environ[\"TWITTER_CONSUMER_SECRET\"]\n",
    "access_token = os.environ[\"TWITTER_ACCESS_TOKEN\"]\n",
    "access_token_secret = os.environ[\"TWITTER_TOKEN_SECRET\"]\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = open(\"Investors_on_twitter.txt\", \"r\")\n",
    "user_list = text_file.readlines()\n",
    "for index, user in enumerate(user_list):\n",
    "    user_list[index]= user.rstrip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name):\n",
    "    '''get all tweets for one user'''\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []\t\n",
    "\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name, count=200, tweet_mode='extended')\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        #print (\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest, tweet_mode='extended')\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "    print (\"{}: {} tweets downloaded so far\".format(screen_name, len(alltweets)))\n",
    "    return alltweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for user in user_list[100:]:\n",
    "    print(user)\n",
    "    if tweets_collection.find({'user': user}).count() > 0:\n",
    "        continue\n",
    "    \n",
    "    user_twitter = api.get_user(user)\n",
    "    description = user_twitter.description\n",
    "    \n",
    "    all_tweets = get_all_tweets(user)\n",
    "    all_tweets_raw = []\n",
    "    for tweet in all_tweets:\n",
    "        all_tweets_raw.append(tweet.full_text)\n",
    "    user_dict = {}\n",
    "    user_dict['user'] = user\n",
    "    user_dict['all_tweets_raw'] = all_tweets_raw\n",
    "    user_dict['description'] = description\n",
    "    tweets_collection.insert_one((user_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "from string import digits\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for investor in tweets_collection.find({}):\n",
    "    all_tweets_clean = []\n",
    "    for tweet in investor['all_tweets_raw']:\n",
    "        tweet_clean = html.unescape(tweet) #convert html entities to string\n",
    "        tweet_clean = ''.join(c for c in unicodedata.normalize('NFC', tweet_clean) if c <= '\\uFFFF') #remove emojis\n",
    "        tweet_clean = re.sub('(#[A-Za-z0-9_]+)|(%)|(@[A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)|(^rt)|(^RT)|(^Rt)|(\\sRT\\s)|(\\sRt\\s)|(\\srt\\s)|(http.+?)', ' ', tweet_clean).strip() #remove %, @, https, RT \n",
    "        tweet_clean = re.sub('\\s+', ' ', tweet_clean) #remove multiple spaces\n",
    "        tweet_clean = ''.join([i for i in tweet_clean if not i.isdigit()]) #remove all digits\n",
    "        all_tweets_clean.append(tweet_clean)\n",
    "    tweets_collection.update_one({\"_id\": investor[\"_id\"]}, {\"$set\": {\"all_tweets_clean\": all_tweets_clean}}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for investor in tweets_collection.find({}):\n",
    "    all_tweets_bow = []\n",
    "    for tweet in investor['all_tweets_clean']:\n",
    "        doc = nlp(tweet)\n",
    "        bow = ''\n",
    "        for token in doc:\n",
    "            if not(token.is_stop or token.is_space or token.is_punct or token.like_num):\n",
    "                bow += token.lemma_ + ' '\n",
    "        all_tweets_bow.append(bow)\n",
    "    tweets_collection.update_one({\"_id\": investor[\"_id\"]}, {\"$unset\":{\"bag_of_words\": \"\"}});\n",
    "    tweets_collection.update_one({\"_id\": investor[\"_id\"]}, {\"$set\": {\"all_tweets_bow\": all_tweets_bow}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for investor in tweets_collection.find({}):\n",
    "    bag_of_words = ''\n",
    "    for tweet in investor['all_tweets_clean']:\n",
    "        doc = nlp(tweet)\n",
    "        for token in doc:\n",
    "            if not(token.is_stop or token.is_space or token.is_punct or token.like_num):\n",
    "                bag_of_words += token.lemma_ + ' '\n",
    "    bag_of_words = re.sub('\\s+', ' ', bag_of_words) #remove multiple spaces\n",
    "    tweets_collection.update_one({\"_id\": investor[\"_id\"]}, {\"$set\": {\"bag_of_words\": bag_of_words}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Macro Trader; tweeting on rates, FX, equities, commods, life's rich pageant...in roughly that order. Not looking to set the world to rights in 140 characters\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_collection.find_one({})['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import textwrap\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from pymongo import MongoClient\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_tweet_df = pd.DataFrame(columns=['user', 'tweet'])\n",
    "for user in user_list:\n",
    "    investor = tweets_collection.find_one({'user': user})\n",
    "    for bow in investor['all_tweets_bow']:\n",
    "        user_tweet_df = user_tweet_df.append(pd.DataFrame([[user, bow]], columns = ['user', 'tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(user_tweet_df, open('user_tweet_df.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans_inertias(ns_clusters, X):\n",
    "    inertias = []\n",
    "    for n_clusters in ns_clusters:\n",
    "        kmeans = KMeans(n_clusters=n_clusters , random_state=random_state).fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    return inertias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans_Sil_coefs(ns_clusters, X):\n",
    "    Sil_coefs = []\n",
    "    for n_clusters in ns_clusters:\n",
    "        kmeans = KMeans(n_clusters=n_clusters , random_state=random_state).fit(X)\n",
    "        labels = kmeans.labels_\n",
    "        Sil_coefs.append(silhouette_score(X, labels, metric='euclidean'))\n",
    "    return Sil_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_hover_text(user_tweet_cluster, pred_cluster):\n",
    "    hover_text = []\n",
    "    for index, row in user_tweet_cluster.iterrows():\n",
    "        user = row['user']\n",
    "        bow = '<br>'.join(textwrap.wrap(row['bow'], 50))\n",
    "        cluster = pred_cluster[index]\n",
    "        hover_text.append(('User: {user}<br>'+\n",
    "                          'Bow: {bow}<br>'+\n",
    "                          'Cluster: {cluster}<br>').format(cluster=cluster,\n",
    "                                                name=user,\n",
    "                                                description=description))\n",
    "    return hover_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_3D(X, hover_text, pred_cluster):\n",
    "    trace1 = go.Scatter3d(\n",
    "        x=X[:,0],\n",
    "        y=X[:,1],\n",
    "        z=X[:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            color=pred_cluster,                # set color to an array/list of desired values\n",
    "            colorscale='Viridis',   # choose a colorscale\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        text=hover_text,\n",
    "\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "\n",
    "    data = [trace1]\n",
    "    layout = go.Layout(\n",
    "        margin=dict(\n",
    "            l=0,\n",
    "            r=0,\n",
    "            b=0,\n",
    "            t=0\n",
    "        )\n",
    "    )\n",
    "    return data, layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "investor_tweets_db = client.investor_tweets_db\n",
    "tweets_collection = investor_tweets_db.tweets_collection\n",
    "text_file = open(\"Investors_on_twitter.txt\", \"r\")\n",
    "user_list = text_file.readlines()\n",
    "for index, user in enumerate(user_list):\n",
    "    user_list[index]= user.rstrip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for user in user_list:\n",
    "    investor = tweets_collection.find_one({'user': user})\n",
    "    corpus.extend(investor['all_tweets_bow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 0.00001, max_df = 0.3, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412601, 22792)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 5\n",
    "lda_model = LatentDirichletAllocation(n_topics=n_topics, max_iter=10, learning_method='online',                \n",
    "                                learning_offset=50, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "investor_topic_matrix = lda_model.fit_transform(X[3500:7000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_word_matrix = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'market', 'high', 'low', 'stock', 'day', 'study', 'spy', 'wix', 'today']\n",
      "['debt', 'tonight', 'bring', 'thought', 'acquire', 'miami', 'chance', 'forget', 'competitor', 'pull']\n",
      "['appreciate', 'player', 'lie', 'yup', 'sq', 'ft', 'profitability', 'bloomberg', 'theater', 'campus']\n",
      "['man', 'lol', 'question', 'save', 'sunday', 'thanks', 'usain', 'skill', 'politic', 'star']\n",
      "['year', 'good', 'great', 'study', 'apple', 'day', 'like', 'time', 'earning', 'company']\n"
     ]
    }
   ],
   "source": [
    "words_in_topics = []\n",
    "for i in range(n_topics):\n",
    "    topic = topic_word_matrix[i, :]\n",
    "    importance = -np.sort(-topic)[:10]\n",
    "    word_indices = (-topic).argsort()[:10]\n",
    "    print([word_list[word_index] for word_index in word_indices])\n",
    "    #print(importance)\n",
    "    words_in_topics.append([word_list[word_index] for word_index in word_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = TfidfTransformer()\n",
    "X_tfidf =  tf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=random_state)\n",
    "svd.fit(X_tfidf)\n",
    "X_reduced = svd.transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explained_variance_ratio = svd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_ratio = 0\n",
    "for index, ratio in enumerate(explained_variance_ratio):\n",
    "    sum_ratio += ratio\n",
    "    if sum_ratio > 0.95:\n",
    "        print(index)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15797616903886358"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd_3 = TruncatedSVD(n_components=3, n_iter=7, random_state=random_state)\n",
    "svd_3.fit(X)\n",
    "X_reduced_3 = svd_3.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0319885316371\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(svd_3.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 22949)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components = svd_3.components_\n",
    "components.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_in_component = []\n",
    "for component in components:\n",
    "    word_indices = (-component).argsort()[:10]\n",
    "    words_in_component.append([word_list[word_index] for word_index in word_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'not', 'good', 'like', 'new', 'time', 'day', 'think', 'year', 'know']\n",
      "['not', 'good', 'new', 'day', 'time', 'like', 'year', 'know', 'great', 'market']\n",
      "['not', 'be', 'know', 'will', 'wait', 'believe', 'understand', 'anymore', 'care', 'mean']\n"
     ]
    }
   ],
   "source": [
    "for component in words_in_component:\n",
    "    print(component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns_clusters = np.arange(2, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inertias = kmeans_inertias(ns_clusters, X_reduced_3) \n",
    "Sil_coefs = kmeans_Sil_coefs(ns_clusters, X_reduced_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(ns_clusters, inertias)\n",
    "plt.xlabel('n_cluster')\n",
    "plt.ylabel('inertia');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(ns_clusters, Sil_coefs)\n",
    "plt.xlabel('n_cluster')\n",
    "plt.ylabel('Sihouette score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6\n",
    "kmeans = KMeans(n_clusters=n, random_state=random_state).fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_cluster = kmeans.fit_predict(X_reduced_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_cluster = make_user_cluster_dataframe(pred_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hover_text = make_hover_text(pred_cluster=pred_cluster, user_list = user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, layout = plot_3D(X_reduced_3, hover_text, user_cluster.cluster)\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "py.iplot(fig, filename='3d-scatter-colorscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
